<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reinforcement Learning on Ashhhi - 杂记</title>
    <link>https://ashhhi.github.io/tags/reinforcement-learning/</link>
    <description>Recent content in Reinforcement Learning on Ashhhi - 杂记</description>
    <generator>Hugo</generator>
    <language>en</language>
    <managingEditor>shijunshen424@gmail.com (Ashhhi)</managingEditor>
    <webMaster>shijunshen424@gmail.com (Ashhhi)</webMaster>
    <lastBuildDate>Wed, 25 Sep 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://ashhhi.github.io/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Bellman Function详解</title>
      <link>https://ashhhi.github.io/tech/2024-9-25-rl02/</link>
      <pubDate>Wed, 25 Sep 2024 00:00:00 +0000</pubDate><author>shijunshen424@gmail.com (Ashhhi)</author>
      <guid>https://ashhhi.github.io/tech/2024-9-25-rl02/</guid>
      <description>&lt;p&gt;本系列为强化学习的学习笔记，本章讲解对State value，Bellman equation的理解。&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;\begin{align*}&#xA;v_\pi(s) &amp;amp;= \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{r \in \mathcal{R}}p(r|s,a)r + \lambda \sum_{s&amp;rsquo; \in \mathcal{S}}v_\pi(s&amp;rsquo;)\sum_{a \in \mathcal{A}} p[s&amp;rsquo;|s,a]\pi(a|s) \\&#xA;&amp;amp;= \sum_{a \in \mathcal{A}}\pi(a|s)[\sum_{r \in \mathcal{R}}p(r|s,a)r+\lambda \sum_{s&amp;rsquo; \in \mathcal{S}}v_\pi(s&amp;rsquo;)p[s&amp;rsquo;|s,a]]&#xA;\end{align*}&#xA;$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>强化学习中的基本概念以及马尔可夫决策</title>
      <link>https://ashhhi.github.io/tech/2024-9-23-rl01/</link>
      <pubDate>Mon, 23 Sep 2024 00:00:00 +0000</pubDate><author>shijunshen424@gmail.com (Ashhhi)</author>
      <guid>https://ashhhi.github.io/tech/2024-9-23-rl01/</guid>
      <description>&lt;p&gt;本系列为强化学习的学习笔记，本章讲解对强化学习的个人理解以及名词解释，如state，action，policy，reward，return，以及MDP。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
